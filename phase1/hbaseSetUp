#this instruction shows starting from a hbase EMR master machine
#how to partition disk and download data to a folder
#how to load data using bulk loading into hbase, get data from it with key
#how to configure, compile and run a java program which talks to hbase
#how to backup hbase data to s3

#not sure if required this aws config
sudo apt-get install python-pip
aws configure
## AWSAccessKeyId=AKIAJWVJUGINPZKLNC5A
## AWSSecretKey=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
## default region = us-east-1
## default format = text


#folder for storing the data
mkdir phase1data
#mount device to the folder, xvdb can also be xvdc
sudo mount /dev/xvdb /home/hadoop/phase1data/
cd /home/hadoop/phase1data/

#cp download.py to the folder
#use download.py to download all s3 data, need to modify download.py for file index and s3 address
#note that python script uses hadoop fs -copyToLocal s3://yourbucket/yourfile as the download command
python download.py
hadoop fs -mkdir /tweets
hadoop fs -put part* /tweets

#enter hbase shell
hbase shell
create 'tweetdata', {NAME => 'data'}
exit
#leave hbase shell

#bulk import using importtsv, separator is tab, colume family is data, colume qualifier is response
#this create HFiles at output output1
hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.bulk.output=output1 -Dimporttsv.columns=HBASE_ROW_KEY,data:response tweetdata /tweets
#completebulkload to load HFile output1 to tweetdata
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles output1 tweetdata

#hbase shell get key '10000572+2014-05-15 11:30:09' from tweet data
hbase shell
get 'tweetdata', '10000572+2014-05-15 11:30:09'
exit

#suppress hbase output, does not really change a lot and does not really matter
#vim /home/hadoop/hbase/conf/log4j.properties
#change log4j.threshold=WARN

#configure classpath and compile java files
export CLASSPATH=/home/hadoop/lib/lib/*:/home/hadoop/hbase/*:/home/hadoop/phase1/
javac -classpath $CLASSPATH *.java
#run program
java -cp $CLASSPATH q2Test

#backup hbase before terminating emr, j-9L9DNYK7YKSN should be correct emr cluster id, phase1elt is bucket name
aws emr create-hbase-backup --cluster-id j-9L9DNYK7YKSN --dir s3://phase1elt/backups/j-9L9DNYK7YKSN